{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabris.dubua.GRADPRAKTIKA\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='SGA_FABRICE_jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)\n",
    "\n",
    "from pyspark.sql.context import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'hdfs:/data/clickstream.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_tab = sqlContext.read.option(\"delimiter\", \"\\t\").option(\"header\", True).csv('clickstream.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|    navigation_route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  676|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  123|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  114|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the timestamp column to integer type  \n",
    "converted_data = sql_tab.withColumn(\"timestamp\", sql_tab.timestamp.cast(\"int\"))  \n",
    "\n",
    "# Find the minimum timestamp for events containing 'error' for each user and session  \n",
    "min_error_ts_df = (  \n",
    "    converted_data  \n",
    "    .filter(converted_data.event_type.contains(\"error\"))  \n",
    "    .groupBy('user_id', 'session_id')  \n",
    "    .agg(F.min('timestamp').alias('min_error_ts'))  \n",
    ")  \n",
    "\n",
    "# Define a window specification to order events by timestamp within each user and session  \n",
    "event_window_spec = Window.partitionBy('user_id', 'session_id').orderBy('timestamp')  \n",
    "\n",
    "# Join the original DataFrame with the DataFrame containing minimum error timestamps  \n",
    "result_df = (  \n",
    "    converted_data  \n",
    "    .join(min_error_ts_df, ['user_id', 'session_id'], 'left')  \n",
    "    .filter(  \n",
    "        (min_error_ts_df.min_error_ts.isNull()) |  \n",
    "        (converted_data.timestamp < min_error_ts_df.min_error_ts)  \n",
    "    )  \n",
    "    .filter(converted_data.event_type == 'page')  \n",
    "    .select('user_id', 'session_id', 'event_page', 'timestamp')  \n",
    "    .withColumn('previous_event_page', F.lag('event_page', 1).over(event_window_spec))  \n",
    "    .filter(  \n",
    "        (F.col('previous_event_page').isNull()) |  \n",
    "        (F.col('event_page') != F.col('previous_event_page'))  \n",
    "    )  \n",
    ")\n",
    "\n",
    "# Collect pages into a list and create the navigation route\n",
    "navigation_route_df = ( \n",
    "    result_df \n",
    "    .groupBy('user_id', 'session_id') \n",
    "    .agg(F.collect_list('event_page').alias('page_list')) \n",
    ")\n",
    "\n",
    "# Create navigation routes by joining pages with '-'\n",
    "navigation_counts_df = ( \n",
    "    navigation_route_df \n",
    "    .select('user_id', 'session_id', F.expr(\"array_join(page_list, '-') as navigation_route\"))  # Use expr for array_join\n",
    "    .groupBy('navigation_route') \n",
    "    .count() \n",
    "    .orderBy(F.desc('count')) \n",
    ") \n",
    "\n",
    "# Show the result \n",
    "navigation_counts_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "|    562|       507|       event|    rabota|1695584181|\n",
      "|    562|       507|       event|    rabota|1695584189|\n",
      "|    562|       507|        page|      main|1695584194|\n",
      "|    562|       507|       event|      main|1695584204|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584219|\n",
      "|    562|       507|        page|     bonus|1695584221|\n",
      "|    562|       507|        page|    online|1695584222|\n",
      "|    562|       507|       event|    online|1695584230|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the existing RDD and schema\n",
    "data_frame = se.createDataFrame(sql_tab.rdd, sql_tab.schema) \n",
    "data_frame.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|    navigation_route|route_count|\n",
      "+--------------------+-----------+\n",
      "|                main|       8184|\n",
      "|        main-archive|       1113|\n",
      "|         main-rabota|       1047|\n",
      "|       main-internet|        897|\n",
      "|          main-bonus|        870|\n",
      "|           main-news|        769|\n",
      "|        main-tariffs|        676|\n",
      "|         main-online|        587|\n",
      "|          main-vklad|        518|\n",
      "| main-rabota-archive|        170|\n",
      "| main-archive-rabota|        167|\n",
      "|  main-bonus-archive|        143|\n",
      "|   main-rabota-bonus|        139|\n",
      "|   main-bonus-rabota|        135|\n",
      "|    main-news-rabota|        135|\n",
      "|main-archive-inte...|        132|\n",
      "|    main-rabota-news|        130|\n",
      "|main-internet-rabota|        129|\n",
      "|   main-archive-news|        126|\n",
      "|main-rabota-internet|        124|\n",
      "|main-internet-arc...|        123|\n",
      "|  main-archive-bonus|        117|\n",
      "| main-internet-bonus|        115|\n",
      "|main-tariffs-inte...|        114|\n",
      "|   main-news-archive|        113|\n",
      "|  main-news-internet|        109|\n",
      "|main-archive-tariffs|        104|\n",
      "|  main-internet-news|        103|\n",
      "|main-tariffs-archive|        103|\n",
      "|    main-rabota-main|         94|\n",
      "+--------------------+-----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'timestamp' column to integer type\n",
    "data_frame_converted = data_frame.withColumn(\"timestamp\", data_frame['timestamp'].cast(\"int\")) \n",
    "\n",
    "# Find the minimum timestamp for events containing 'error' for each user and session\n",
    "min_timestamp_df = (\n",
    "    data_frame_converted\n",
    "    .select(\"user_id\", \"session_id\", \"timestamp\")\n",
    "    .filter(data_frame_converted.event_type.contains(\"error\"))\n",
    "    .groupBy('user_id', 'session_id')\n",
    "    .agg(F.min('timestamp').alias('min_timestamp')) \n",
    ")\n",
    "\n",
    "# Define a window specification to order events by timestamp within each user and session\n",
    "window_spec = Window.partitionBy('user_id', 'session_id').orderBy('timestamp') \n",
    "\n",
    "# Process the converted DataFrame to find navigation routes\n",
    "navigation_routes_df = (\n",
    "    data_frame_converted\n",
    "    .join(min_timestamp_df, ['user_id', 'session_id'], 'left') \n",
    "    .filter((min_timestamp_df.min_timestamp.isNull()) | (data_frame_converted.timestamp < min_timestamp_df.min_timestamp)) \n",
    "    .filter(data_frame_converted.event_type == 'page') \n",
    "    .select('user_id', 'session_id', 'event_page', 'timestamp') \n",
    "    .withColumn('previous_event_page', F.lag('event_page', 1).over(window_spec)) \n",
    "    .filter((F.col('previous_event_page').isNull()) | (F.col('event_page') != F.col('previous_event_page'))) \n",
    "    .groupBy('user_id', 'session_id') \n",
    "    .agg(F.collect_list(data_frame_converted.event_page).alias('pages_list')) \n",
    "    .groupBy('user_id', 'session_id', 'pages_list') \n",
    "    .agg(F.array_join('pages_list', '-').alias('navigation_route')) \n",
    "    .select('navigation_route') \n",
    "    .groupBy('navigation_route') \n",
    "    .agg(F.count('*').alias('route_count')) \n",
    "    .orderBy(F.desc('route_count'))\n",
    ")\n",
    "\n",
    "# Display the top 30 navigation routes\n",
    "navigation_routes_df.show(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to an RDD\n",
    "rdd_events = sql_tab.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for error events, extract user-session pairs with their timestamps,\n",
    "# and find the minimum timestamp for each user-session pair\n",
    "error_events_map = (\n",
    "    rdd_events\n",
    "    .filter(lambda event: 'error' in event['event_type'])  # Keep only error events\n",
    "    .map(lambda event: (f\"{event['user_id']}_{event['session_id']}\", int(event['timestamp'])))  # Create (user_session, timestamp) pairs\n",
    "    .groupByKey()  # Group by user-session key\n",
    "    .map(lambda user_session: (user_session[0], min(user_session[1])))  # Find minimum timestamp for each user-session\n",
    "    .collectAsMap()  # Collect results as a dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 items of the error events dictionary\n",
    "list(error_events_map.items())[:10]\n",
    "\n",
    "# Broadcast the error events dictionary to all nodes in the cluster\n",
    "broadcast_error_events = sc.broadcast(error_events_map)  # Send to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import groupby\n",
    "\n",
    "# Function to remove consecutive duplicate pages from a session\n",
    "def remove_consecutive_duplicates(user_session):\n",
    "    # Sort the pages based on their timestamps\n",
    "    sorted_pages = [page[0] for page in sorted(user_session[1], key=lambda entry: entry[1])]\n",
    "    # Use groupby to extract unique consecutive pages\n",
    "    unique_pages = [key for key, _ in groupby(sorted_pages)]\n",
    "    return (user_session[0], unique_pages)\n",
    "\n",
    "# Process the main data to filter and create the desired output\n",
    "filtered_sessions = (\n",
    "    rdd_events\n",
    "    .map(lambda entry: (f\"{entry['user_id']}_{entry['session_id']}\", [entry['event_type'], entry['event_page'], int(entry['timestamp'])]))  # Create user-session pairs with event details\n",
    "    .filter(lambda entry: entry[0] not in broadcast_error_events.value or entry[1][2] < broadcast_error_events.value[entry[0]])  # Exclude error events\n",
    "    .filter(lambda entry: \"page\" == entry[1][0])  # Keep only page events\n",
    "    .map(lambda entry: (entry[0], (entry[1][1], entry[1][2])))  # Map to (user_session, (page, timestamp))\n",
    "    .groupByKey()  # Group by user-session\n",
    "    .map(remove_consecutive_duplicates)  # Remove consecutive duplicates\n",
    "    .map(lambda session: (session[0], \"-\".join(list(session[1]))))  # Join pages into a single string\n",
    "    .map(lambda session: (session[1], 1))  # Prepare for counting occurrences\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)  # Count occurrences of each unique page sequence\n",
    "    .sortBy(lambda session: session[1], ascending=False)  # Sort by counts in descending order\n",
    "    .take(30)  # Take the top 30 results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('main', 8184),\n",
       " ('main-archive', 1113),\n",
       " ('main-rabota', 1047),\n",
       " ('main-internet', 897),\n",
       " ('main-bonus', 870),\n",
       " ('main-news', 769),\n",
       " ('main-tariffs', 677),\n",
       " ('main-online', 587),\n",
       " ('main-vklad', 518),\n",
       " ('main-rabota-archive', 170),\n",
       " ('main-archive-rabota', 167),\n",
       " ('main-bonus-archive', 143),\n",
       " ('main-rabota-bonus', 139),\n",
       " ('main-news-rabota', 135),\n",
       " ('main-bonus-rabota', 135),\n",
       " ('main-archive-internet', 132),\n",
       " ('main-rabota-news', 130),\n",
       " ('main-internet-rabota', 129),\n",
       " ('main-archive-news', 126),\n",
       " ('main-rabota-internet', 124),\n",
       " ('main-internet-archive', 123),\n",
       " ('main-archive-bonus', 117),\n",
       " ('main-internet-bonus', 115),\n",
       " ('main-tariffs-internet', 114),\n",
       " ('main-news-archive', 113),\n",
       " ('main-news-internet', 109),\n",
       " ('main-archive-tariffs', 104),\n",
       " ('main-internet-news', 103),\n",
       " ('main-tariffs-archive', 103),\n",
       " ('main-rabota-main', 94)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sessions  # Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the top 10 results\n",
    "res = {}\n",
    "for k, v in filtered_sessions[:10]:\n",
    "    res[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "result_SGA_Fabrice = json.dumps(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"result_SGA_Fabrice.json\", \"w\")\n",
    "f.write(result_SGA_Fabrice)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
