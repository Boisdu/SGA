{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides a more flexible and efficient way to perform distributed data processing compared to traditional MapReduce. Here is an outline of the algorithm to perform this conversion along with some tips for potential speed improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We analyze the MapReduce task:\n",
    "\n",
    "   • Identify the input format (e.g., text, CSV).\n",
    "\n",
    "   • Understand the Mapper and Reducer logic.\n",
    "\n",
    "   • Determine any intermediate steps like Combiner or sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We set up the Spark Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \n",
    "    .appName(\"MapReduceToSpark\") \n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We load the input Data:\n",
    "\n",
    "In MapReduce, data is typically read using InputFormat. In Spark, you can use SparkContext or SparkSession to read data from various sources (e.g., HDFS, S3, local files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MapReduceToSpark\").getOrCreate()\n",
    "data = spark.read.text(\"hdfs://path/to/input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We implement the Map Function by replacing the Mapper with a map() transformation.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = df.rdd.map(lambda line: process_line(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We implement the shuffle phase using reduceByKey() or groupByKey()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = mapped_data.reduceByKey(lambda a, b: a + b)  # This an example of aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Implement the Reduce Function by using reduce() or aggregate() for the Reducer logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_result = reduced_data.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. If necessary we can persist intermediate results:\n",
    "\n",
    "Cache RDDs or DataFrames that are reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Finally we output the results:\n",
    "\n",
    "We write the final output using Spark's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.saveAsTextFile(\"hdfs://path/to/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed improvments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here I provide some speed improvement tips:\n",
    "\n",
    "1. The use of DataFrames and Datasets: \n",
    "\n",
    "They provide optimizations like Catalyst optimizer and Tungsten execution engine which can significantly speed up queries.\n",
    "\n",
    "2. Avoid shuffles when possible:\n",
    "\n",
    "Shuffling is expensive; we can try to minimize it by using transformations that require fewer shuffles.\n",
    "\n",
    "3. Broadcast Variables:\n",
    "\n",
    "If we have large datasets that need to be shared across tasks, consider using broadcast variables to avoid sending large amounts of data over the network.\n",
    "\n",
    "4. Optimize Partitioning:\n",
    "\n",
    "Adjust the number of partitions based on the size of our data and cluster resources to ensure optimal parallelism.\n",
    "\n",
    "5. Use Efficient File Formats:\n",
    "\n",
    "We can use columnar formats like Parquet or ORC for better I/O performance.\n",
    "\n",
    "6. Leverage Caching:\n",
    "\n",
    "We can cache intermediate results if they will be reused, especially in iterative algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
